{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple implementation of Ensemble learning Based Spark"
      ],
      "metadata": {
        "id": "0-xkJlmDHvWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ability to mine data for potential value has been greatly aided by machine learning in recent years. To offer more effective machine learning service capabilities, big Internet companies typically create distributed machine learning platforms. However, the majority of ensemble learning in machine learning today is only stand-alone, which has drawbacks like slow hardware performance improvement, high time costs, and memory inability to handle large data sets. Based on massive data sets, distributed ensemble learning can significantly increase machine learning efficiency and lower time costs. It increases the effectiveness of ensemble learning in situations involving large amounts of data. In this paper, the random forest and optimal subset algorithms are implemented using the Spark Resilient Distributed Dataset, while the decision tree is utilized as the base learner to extend the Bagging and Boosting algorithms to the distributed computing engine Spark. With runtime, speedup and F1-score as evaluation indexes, comparative experiments were carried out on different a number of data sets and node numbers. The results show that F1-score is basically unchanged, and the runtime and speedup of the two algorithms are basically linear with the increase of node number. This shows that the distributed algorithm greatly reduces the time cost when the accuracy is basically unchanged. The parallel algorithm based on spark has good scalability and can support experiments on larger data sets. In machine learning, ensemble learning is a subset of supervised algorithms. It uses an algorithm to produce several weakly supervised models, which are then combined in a specific way to produce a better and more complete strongly supervised mode. The equivalency between weak and strong learning algorithms was then first proposed by Kearns and Valiant.The voting method and the average method are currently the most widely used algorithms for combined models. Models that perform better in classification, prediction, and function estimation can benefit from ensemble learning."
      ],
      "metadata": {
        "id": "86MdQ2r4IFoR"
      }
    }
  ]
}